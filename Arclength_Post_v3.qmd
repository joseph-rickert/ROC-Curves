---
title: "Arclength Post"
subtitle: ""
author: Joseph Rickert
date: November 29, 2025
format: html
---

This post is an attempt to make the case for using the arclength of a smoothed ROC curve as a complementary to Area Under the Curve (AUC) as a measure of classifier performance. The main insight is that ROC curve is the direct representation of the conditional probability distribution related to True Positive Rate (TPR) and False Positive Rate (FPR). Each point (x,y) on the ROC curve equals $P(TPR \le y \mid FPR \le x)$. This a direct, and to my mind clear, probability statement. AUC is an indirect measure with the probability interpretation usually rendered something like the definition provided in [wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#:~:text=The%20area%20under%20the%20curve,ranks%20higher%20than%20'negative')

>"AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative')'. 

I don't see how this is useful.

It is also the case that arclength is a linear measure while AUC is an area. I may very well be be wrong about this, but I think most people have a better intuition of the practical significance of a linear difference of 0.3 than an area difference of 0.3.

Finally, arclength is directly related to the geometry of the ROC curve and some basic ideas of differential geometry may be useful in interpreting the behavior of ROC curves.

## Part 1: a basic example

### 1. Load the Required Packages
```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show the packages required"
library(tidymodels) # For modeling and evaluation
library(dplyr)    # For data manipulation
library(ggplot2)  # For plotting
library(MASS) # for Pima.tr
library(mlbench) # for data
library(broom)
library(pROC)  # For ROC curve analysis
tidymodels_prefer()

```


### Some suitable data sets from R packages

In this section I have three suitable data sets from R packages and put the data, abstracted just two features and put them in a form suitable for some simple tidy modeling.
```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
# Load a sample dataset (e.g., `two_class_dat` from `modeldata`)
data(two_class_dat, package = "modeldata")
two_class_dat2 <- two_class_dat %>%  mutate(Class = recode(Class,
                           "Class1" = "1",
                           "Class2" = "2"))

data(Pima.tr, package = "MASS")
Pima.tr2 <- Pima.tr %>% mutate(
                              Class = type,
                              Class = recode(Class,
                              "Yes" = "2",
                              "No" = "1")) %>%
                        select(c(bmi,bp,Class))

data(aSAH, package = "pROC")
aSAH2 <- aSAH %>% mutate(
                              Class = outcome,
                              Class = recode(Class,
                              "Good" = "1",
                              "Poor" = "2")) %>%
                        select(c(s100b,ndka,Class))

```

### Select the Data Set

The code in this section selects one of the data sets and prepares it for classification.
```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
# Set a seed for reproducibility
set.seed(123)

# Select a data set

#df<- two_class_dat2 # try FPC = 1, FNC = 10
#df <- Pima.tr2 # try FPC = 10 , FNC = 1
df <- aSAH2 #try FPC = 1, FNC = 1
head(df)

# Split the data into training and testing sets
data_split <- initial_split(df, prop = 0.75, strata = Class)
train_data <- training(data_split)
test_data <- testing(data_split)

```

The data frame has two features and a class label.

### Set up the tidymodels workflows for three models: logistic regression, SVM, and decision tree
```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
# Define the models

# 1. Logistic Regression
log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# 2. Support Vector Machine (SVM)
svm_spec <- svm_linear() %>%
  set_engine("kernlab") %>%
  set_mode("classification")

# 3. Decision Tree
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# Create workflows for each model
log_reg_wf <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_formula(Class ~ .)

svm_wf <- workflow() %>%
  add_model(svm_spec) %>%
  add_formula(Class ~ .)

tree_wf <- workflow() %>%
  add_model(tree_spec) %>%
  add_formula(Class ~ .)

# Fit the models to the training data
log_reg_fit <- fit(log_reg_wf, data = train_data)
svm_fit <- fit(svm_wf, data = train_data)
tree_fit <- fit(tree_wf, data = train_data)

# Collect predictions for each model on the test data
log_reg_preds <- predict(log_reg_fit, new_data = test_data, type = "prob") %>%
  bind_cols(test_data %>% select(Class)) %>%
  mutate(model = "Logistic Regression")

svm_preds <- predict(svm_fit, new_data = test_data, type = "prob") %>%
  bind_cols(test_data %>% select(Class)) %>%
  mutate(model = "SVM")

tree_preds <- predict(tree_fit, new_data = test_data, type = "prob") %>%
  bind_cols(test_data %>% select(Class)) %>%
  mutate(model = "Decision Tree")

# Compute AUCs and relabel models with AUC values
auc_tree <- roc_auc(tree_preds, truth = Class, .pred_1)$.estimate
auc_svm <- roc_auc(svm_preds, truth = Class, .pred_1)$.estimate
auc_log_reg <- roc_auc(log_reg_preds, truth = Class, .pred_1)$.estimate

# Update model labels to include AUC
log_reg_preds <- log_reg_preds %>%
  mutate(model = paste0("Logistic Regression (AUC = ", round(auc_log_reg, 3), ")"))

svm_preds <- svm_preds %>%
  mutate(model = paste0("SVM (AUC = ", round(auc_svm, 3), ")"))

tree_preds <- tree_preds %>%
  mutate(model = paste0("Decision Tree (AUC = ", round(auc_tree, 3), ")"))

# Combine predictions with updated labels
all_preds <- bind_rows(log_reg_preds, svm_preds, tree_preds)
```

## Plot the Raw ROC CUrves
```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
### Plot the ROC Curves

 all_preds %>%
   group_by(model) %>%
  roc_curve(truth = Class, .pred_1) %>%
  autoplot() +
  labs(
    title = "ROC Curves for Multiple Classifiers",
    color = "Model"
  ) 


# Make a copy of all_preds with cleaned model names
all_preds_2 <- all_preds %>%
    mutate(model_AUC = model,
              model= sub(" \\(AUC.*$", "", model_AUC))


# Compute ROC data for each model
roc_data <- all_preds_2 %>%
  group_by(model) %>%
  roc_curve(truth = Class, .pred_1)

# Compute AUC for each model
auc_data <- all_preds_2 %>%
  group_by(model) %>%
  roc_auc(truth = Class, .pred_1)

# Inspect actual model names
#print(unique(roc_data$model))

# Build legend labels with AUC
legend_labels <- paste0(auc_data$model,
                        " (AUC = ", sprintf("%.3f", auc_data$.estimate), ")")

# IMPORTANT: match colors to the actual values in your data
# Replace the strings below with the exact output from unique(roc_data$model)
color_values <- c(
  "Logistic Regression" = "#1b9e77",
  "SVM"                 = "#7570b3",
  "Decision Tree"       = "#d95f02"
)


# Ensure model is a factor with levels matching auc_data$model
roc_data <- roc_data %>%
  mutate(model = factor(model, levels = auc_data$model))

# Plot raw step-function ROC curves
ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(size = 1.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey50") +
  coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1), expand = FALSE) +
  labs(title = "ROC Curves for Multiple Classifiers",
       x = "False Positive Rate (FPR)",
       y = "True Positive Rate (TPR)",
       color = NULL) +
  scale_color_manual(values = color_values, labels = legend_labels) +
  #theme_classic() +
  theme(
    legend.position = c(0.65, 0.25),
    legend.text = element_text(size = 9, lineheight = 1.1),
    legend.background = element_rect(fill = alpha("white", 0.7), color = NA),
    plot.margin = margin(20, 20, 20, 20)
  )

```

### Compute and Plot Smoothed ROC Curves with AUC and Arc Length

```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"

df <- all_preds
trapz <- function(x, y) {
  sum((y[-1] + y[-length(y)]) / 2 * diff(x))
}

discrete_roc <- function(df) {
  roc_obj <- roc(response = df$Class,
                 predictor = df$.pred_1,
                 levels = c("2","1"),   # control first, positive second
                 direction = "<")
  
  rc <- coords(roc_obj, "all", ret = c("specificity","sensitivity"), transpose = FALSE)
  FPR <- 1 - rc$specificity
  TPR <- rc$sensitivity
  
  FPR <- c(0, FPR, 1)
  TPR <- c(0, TPR, 1)
  ord <- order(FPR, TPR)
  FPR <- FPR[ord]
  TPR <- cummax(TPR[ord])   # enforce monotonicity
  
  tibble(FPR = FPR, TPR = TPR)
}

smooth_roc <- function(FPR, TPR, n = 400) {
  df <- tibble(FPR = FPR, TPR = TPR) %>% arrange(FPR) %>% distinct(FPR, .keep_all = TRUE)
  mono_fun <- splinefun(x = df$FPR, y = df$TPR, method = "monoH.FC")
  x <- seq(0, 1, length.out = n)
  y <- pmin(pmax(mono_fun(x), 0), 1)
  
  auc <- trapz(x, y)
  dy <- mono_fun(x, deriv = 1)
  arc <- trapz(x, sqrt(1 + dy^2))
  
  tibble(FPR = x, TPR = y, auc = auc, arc = arc)
}

# Normalize model names
 df <- df %>%
  mutate(model_norm = sub(" \\(.*$", "", model))

smooth_results <- df %>%
  group_by(model_norm) %>%
  group_modify(~ {
    dr <- discrete_roc(.x)
    sr <- smooth_roc(dr$FPR, dr$TPR, n = 400)
    #sr %>% mutate(model_norm = unique(.x$model_norm))
  }) %>%
  ungroup()

metrics <- smooth_results %>%
  group_by(model_norm) %>%
  summarise(AUC = unique(auc), Arc = unique(arc), .groups = "drop")

# legend_labels <- setNames(
#   paste0(metrics$model_norm, "\nAUC = ", sprintf("%.3f", metrics$AUC),
#          "\nArc = ", sprintf("%.3f", metrics$Arc)),
#   metrics$model_norm
# )
# 
# color_values <- setNames(
#   c( "#d95f02", "#1b9e77","#7570b3"),
#   unique(metrics$model_norm)
# )
# 
# p_smooth <- ggplot(smooth_results, aes(x = FPR, y = TPR, color = model_norm)) +
#   geom_line(linewidth = 1.4) +
#   geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey50") +
#   coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1), expand = FALSE) +  # square plot
#   labs(title = "Smoothed ROC curves by model",
#        x = "False positive rate (FPR)",
#        y = "True positive rate (TPR)",
#        color = NULL) +
#   scale_color_manual(values = color_values, labels = legend_labels) +
#   #theme_minimal(base_size = 16) +
#   theme(
#     legend.position.inside = c(0.65, 0.25),   # inside plot, under diagonal
#     legend.text = element_text(size = 9, lineheight = 1.1),
#     legend.background = element_rect(fill = alpha("white", 0.7), color = NA),
#     plot.margin = margin(20, 20, 20, 20)
#   )
# p_smooth
```



## Overlay of Raw and Smooth ROC curves



```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"

# df_raw: columns .pred_1, .pred_2, Class, model
# smooth_results: columns model_norm, FPR, TPR, auc, arc

# 1) Normalize model names in the raw data so they match smooth_results$model_norm
df_raw <- all_preds %>%
  mutate(model_norm = sub(" \\(.*$", "", model))  # e.g., "Logistic Regression (AUC = ...)" -> "Logistic Regression"

# 2) Compute raw ROC coordinates and raw AUC per model
raw_results <- df_raw %>%
  group_by(model_norm) %>%
  group_map(~{
    roc_obj <- roc(response = .x$Class,
                   predictor = .x$.pred_1,
                   levels = c("2","1"),
                   direction = "<")

    rc <- coords(roc_obj, "all", ret = c("specificity","sensitivity"), transpose = FALSE)
    FPR <- 1 - rc$specificity
    TPR <- rc$sensitivity

    # pad ends, order, enforce monotone TPR
    FPR <- c(0, FPR, 1)
    TPR <- c(0, TPR, 1)
    ord <- order(FPR, TPR)
    FPR <- FPR[ord]
    TPR <- cummax(TPR[ord])

    auc_raw <- as.numeric(auc(roc_obj))
    m <- .y$model_norm[[1]]  # group label; safer than looking back into .x

    tibble(
      model_norm = rep(m, length(FPR)),
      FPR = FPR,
      TPR = TPR,
      auc_raw = rep(auc_raw, length(FPR)),
      curve_type = rep("raw", length(FPR))
    )
  }) %>%
  bind_rows()

# 3) Prepare smoothed results to match columns (add curve_type and placeholder auc_raw)
smooth_results_plot <- smooth_results %>%
  mutate(
    curve_type = "smooth",
    auc_raw = NA_real_  # placeholder so bind_rows columns align
  )

# 4) Combine raw + smooth results
roc_results <- bind_rows(raw_results, smooth_results_plot)

# 5) Build legend metrics: Raw AUC (from raw_results), Smooth AUC and Arc (from smooth_results)
metrics_raw <- raw_results %>%
  distinct(model_norm, auc_raw) %>%
  rename(AUC_raw = auc_raw)

metrics_smooth <- smooth_results %>%
  distinct(model_norm, auc, arc) %>%
  rename(AUC_smooth = auc, Arc = arc)

metrics <- metrics_raw %>%
  left_join(metrics_smooth, by = "model_norm")

legend_labels <- setNames(
  paste0(metrics$model_norm,
         "\nRaw AUC = ", sprintf("%.3f", metrics$AUC_raw),
         "\nSmooth AUC = ", sprintf("%.3f", metrics$AUC_smooth),
         "\nArc = ", sprintf("%.3f", metrics$Arc)),
  metrics$model_norm
)

# 6) Pastel colors (keep model association). Ensure names match smooth_results$model_norm exactly.
color_values <- c(
  "Decision Tree"        = "#fcae91",  # pastel red
  "Logistic Regression"  = "#a1d99b",  # pastel green
  "SVM"                  = "#9ecae1"   # pastel blue
)



legend_labels <- setNames(
  paste0(metrics$model_norm,
         "\nRaw AUC = ", sprintf("%.3f", metrics$AUC_raw),
         " | Smooth AUC = ", sprintf("%.3f", metrics$AUC_smooth),
         "\nArc = ", sprintf("%.3f", metrics$Arc),
         "\n"),   # blank line between models
  metrics$model_norm
)

ggplot(roc_results, aes(x=FPR, y=TPR,
                        color=model_norm,
                        linetype=curve_type)) +
  geom_line(linewidth=1) +
  geom_abline(slope=1, intercept=0,
              linetype="dashed", color="grey60") +
  scale_color_manual(values=color_values, labels=legend_labels) +
  scale_linetype_manual(
    values = c("raw"="dashed", "smooth"="solid"),
    labels = c("raw"="Raw ROC: dashed", "smooth"="Smoothed ROC: solid")
  ) +
  scale_x_continuous(limits=c(0,1), expand=c(0,0)) +
  scale_y_continuous(limits=c(0,1), expand=c(0,0)) +
  labs(title="ROC curves by model (raw vs smoothed)",
       x="False Positive Rate",
       y="True Positive Rate",
       color=NULL, linetype=NULL) +
  guides(
    color = guide_legend(order = 1, title = NULL, label.theme = element_text(size = 7)),
    linetype = guide_legend(order = 2, title = NULL, label.theme = element_text(size = 7))
  ) +
  theme_minimal(base_size=10) +
  theme(
    plot.title = element_text(size=9, face="bold"),
    axis.title = element_text(size=8),
    axis.text  = element_text(size=7),
    legend.position = c(0.65, 0.25),   # inside plot, under diagonal
    legend.text = element_text(size=7, lineheight=1.2),
    legend.background = element_rect(fill = alpha("white", 0.8), color = NA),
    legend.key.size = unit(0.5, "lines"),
    plot.margin = margin(2, 2, 2, 2)
  )


```


```{r}
library(dplyr)
library(ggplot2)
library(purrr)
library(patchwork)
library(tibble)

# --- Palette: pastel per model ---
model_colors <- c(
  "Decision Tree"       = "#F28E8E",  # pastel red
  "Logistic Regression" = "#8FD19E",  # pastel green
  "SVM"                 = "#8EB8FF"   # pastel blue
)

# --- Clean ROC per group: sort, drop duplicate FPR, enforce monotone TPR ---
clean_roc <- function(df) {
  df %>%
    arrange(FPR) %>%
    distinct(FPR, .keep_all = TRUE) %>%
    mutate(
      FPR = pmin(pmax(FPR, 0), 1),
      TPR = pmin(pmax(cummax(TPR), 0), 1)
    )
}

# --- Spline builder (use "natural" to avoid strict monotonicity errors) ---
make_spline <- function(df) {
  df <- clean_roc(df)
  splinefun(x = df$FPR, y = df$TPR, method = "natural")
}

# --- Geometry over a uniform grid: y, y', y'' ---
compute_roc_geometry <- function(df, n_grid = 1001) {
  f <- make_spline(df)
  xg  <- seq(0, 1, length.out = n_grid)
  yg  <- f(xg, deriv = 0)
  y1g <- f(xg, deriv = 1)
  y2g <- f(xg, deriv = 2)
  tibble(FPR = xg, TPR = yg, dTPR = y1g, d2TPR = y2g)
}

# --- Plotters with small text and single-color line per model ---
plot_roc <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = TPR)) +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(
      title = paste(model_label, "â€” Smoothed ROC"),
      x = "FPR",
      y = "TPR"
    ) +
    coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 8),
      panel.grid.minor = element_blank()
    )
}

plot_derivative <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = dTPR)) +
    geom_hline(yintercept = 0, color = "grey80") +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(
      title = paste(model_label, "â€” dTPR/dFPR vs FPR"),
      x = "FPR",
      y = "dTPR/dFPR"
    ) +
    coord_cartesian(xlim = c(0, 1)) +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 8),
      panel.grid.minor = element_blank()
    )
}

# --- Main: build 3x2 grid with ROC on the LEFT, derivative on the RIGHT ---
plot_roc_geometry_grid <- function(smooth_results, n_grid = 1001) {
  # Ensure intended order of rows
  model_order <- c("Decision Tree", "Logistic Regression", "SVM")
  models <- intersect(model_order, unique(smooth_results$model_norm))
  stopifnot(length(models) > 0)

  rows <- map(models, function(m) {
    df <- smooth_results %>% filter(model_norm == m)
    geom_df <- compute_roc_geometry(df, n_grid)
    col_hex <- model_colors[[m]]
    p_left  <- plot_roc(geom_df, m, col_hex)
    p_right <- plot_derivative(geom_df, m, col_hex)
    p_left | p_right
  })

  # Stack rows into 3x2 (or as many as available)
  reduce(rows, `/`)
}

# --- Usage ---
# smooth_results must have columns: model_norm, FPR, TPR
grid_plot <- plot_roc_geometry_grid(smooth_results, n_grid = 2001)
print(grid_plot)

```







### Table Comparing Arclength with AUC 

| Aspect                        | AUC (Area Under Curve)                                                                 | Arc Length (ROC Curve Length)                                                                 |
|-------------------------------|-----------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| **Definition**                | Integral of TPR over FPR: $\int_0^1 f(x)\,dx$                                        | Integral of curve length: $\int_0^1 \sqrt{1+(f'(x))^2}\,dx$                                  |
| **Probabilistic meaning**     | Probability that a randomly chosen positive is ranked above a randomly chosen negative | Probability that TPR â‰¤ y given FPR â‰¤ x (joint distribution along ROC trajectory)               |
| **Bounds**                    | 0.5 (random) to 1.0 (perfect)                                                          | \(\sqrt{2} \approx 1.414\) (diagonal) to 2.0 (perfect staircase ROC)                           |
| **Interpretability**          | Widely used, intuitive for clinicians; benchmarks exist (e.g., >0.9 = excellent)        | Linear measure, easier to visualize by eye; highlights curve geometry and trajectory           |
| **Sensitivity to curve shape**| Less sensitive â€” curves with different shapes can yield similar AUC                     | More sensitive â€” captures slope changes, curvature, and smoothness differences                 |
| **Partial evaluation**        | Partial AUC requires normalization; less visually obvious                               Arclength avoids the problematic region for most reasonable ROC curves                        |
| **Noise robustness**          | Relatively robust; integrates over curve                                                | More sensitive to noise or jaggedness; small oscillations inflate length                       |
| **Clinical adoption**         | Standard metric with established thresholds                                             | Novel metric; not yet widely adopted, requires new benchmarks                                  |
| **Use cases**                 | Ranking accuracy, overall discrimination power                                          | Diagnostic trajectory, geometric comparison, highlighting regional performance differences     |





#### Derivative with respect to FPR points

```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
df <- all_preds

trapz <- function(x, y) {
  sum((y[-1] + y[-length(y)]) / 2 * diff(x))
}

discrete_roc <- function(df) {
  roc_obj <- roc(response = df$Class,
                 predictor = df$.pred_1,
                 levels = c("2","1"),   # control first, positive second
                 direction = "<")
  
  rc <- coords(roc_obj, "all", ret = c("specificity","sensitivity"), transpose = FALSE)
  FPR <- 1 - rc$specificity
  TPR <- rc$sensitivity
  
  FPR <- c(0, FPR, 1)
  TPR <- c(0, TPR, 1)
  ord <- order(FPR, TPR)
  FPR <- FPR[ord]
  TPR <- cummax(TPR[ord])   # enforce monotonicity
  
  tibble(FPR = FPR, TPR = TPR)
}

# Modified smoother: also compute derivative of TPR wrt FPR
smooth_roc <- function(FPR, TPR, n = 400) {
  df <- tibble(FPR = FPR, TPR = TPR) %>% arrange(FPR) %>% distinct(FPR, .keep_all = TRUE)
  mono_fun <- splinefun(x = df$FPR, y = df$TPR, method = "monoH.FC")
  
  x <- seq(0, 1, length.out = n)
  y <- pmin(pmax(mono_fun(x), 0), 1)
  
  auc <- trapz(x, y)
  dy <- mono_fun(x, deriv = 1)   # derivative dTPR/dFPR
  arc <- trapz(x, sqrt(1 + dy^2))
  
  tibble(FPR = x,
         TPR = y,
         auc = auc,
         arc = arc,
         dTPR_dFPR = dy)   # new column: derivative
}

# Normalize model names
df <- df %>%
  mutate(model_norm = sub(" \\(.*$", "", model))

smooth_results <- df %>%
  group_by(model_norm) %>%
  group_modify(~ {
    dr <- discrete_roc(.x)
    smooth_roc(dr$FPR, dr$TPR, n = 400)
  }) %>%
  ungroup()

metrics <- smooth_results %>%
  group_by(model_norm) %>%
  summarise(AUC = unique(auc), Arc = unique(arc), .groups = "drop")

legend_labels <- setNames(
  paste0(metrics$model_norm, "\nAUC = ", sprintf("%.3f", metrics$AUC),
         "\nArc = ", sprintf("%.3f", metrics$Arc)),
  metrics$model_norm
)

color_values <- setNames(
  c("#d95f02", "#1b9e77", "#7570b3"),
  unique(metrics$model_norm)
)

# Plot ROC curves
# ggplot(smooth_results, aes(x = FPR, y = TPR, color = model_norm)) +
#   geom_line(linewidth = 1.4) +
#   geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey50") +
#   coord_fixed(ratio = 1, xlim = c(0,1), ylim = c(0,1), expand = FALSE) +
#   labs(title = "Smoothed ROC curves by model",
#        x = "False positive rate (FPR)",
#        y = "True positive rate (TPR)",
#        color = NULL) +
#   scale_color_manual(values = color_values, labels = legend_labels) +
#   theme(
#     legend.position.inside = c(0.65, 0.25),
#     legend.text = element_text(size = 9, lineheight = 1.1),
#     legend.background = element_rect(fill = alpha("white", 0.7), color = NA),
#     plot.margin = margin(20, 20, 20, 20)
#   )

# Optional: plot derivative curves
ggplot(smooth_results, aes(x = FPR, y = dTPR_dFPR, color = model_norm)) +
  geom_line(linewidth = 1.2) +
  labs(title = "Derivative of ROC curve (dTPR/dFPR)",
       x = "False positive rate (FPR)",
       y = "dTPR/dFPR",
       color = "Model") +
  theme_classic(base_size = 14)


```


## Curvature


$y'(x) = \frac{d\,\text{TPR}}{d\,\text{FPR}}$

$y'(x) = \frac{d\,\text{TPR}}{d\,\text{FPR}}$

$y''(x) = \frac{d^2\,\text{TPR}}{d\,\text{FPR}^2}$

$y'''(x) = \frac{d^3\,\text{TPR}}{d\,\text{FPR}^3}$

$\kappa(x) = \frac{|y''(x)|}{\left(1 + \left[y'(x)\right]^2\right)^{3/2}}$

$\frac{d\kappa}{dx} = \frac{y'''(x)\,\left(1 + \left[y'(x)\right]^2\right) - 3\,y'(x)\,\left(y''(x)\right)^2}{\left(1 + \left[y'(x)\right]^2\right)^{5/2}}$


```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
# --- Palette: pastel per model ---
model_colors <- c(
  "Decision Tree"       = "#F28E8E",  # pastel red
  "Logistic Regression" = "#8FD19E",  # pastel green
  "SVM"                 = "#8EB8FF"   # pastel blue
)

# --- Clean ROC per group: sort, drop duplicate FPR, enforce monotone TPR ---
clean_roc <- function(df) {
  df %>%
    arrange(FPR) %>%
    distinct(FPR, .keep_all = TRUE) %>%
    mutate(
      FPR = pmin(pmax(FPR, 0), 1),
      TPR = pmin(pmax(cummax(TPR), 0), 1)
    )
}

# --- Spline builder (use "natural" to avoid strict monotonicity errors) ---
make_spline <- function(df) {
  df <- clean_roc(df)
  splinefun(x = df$FPR, y = df$TPR, method = "natural")
}

# --- Geometry over a uniform grid: y, y', y'', curvature ---
compute_roc_geometry <- function(df, n_grid = 1001) {
  f <- make_spline(df)
  xg  <- seq(0, 1, length.out = n_grid)
  yg  <- f(xg, deriv = 0)
  y1g <- f(xg, deriv = 1)
  y2g <- f(xg, deriv = 2)
  kappa <- abs(y2g) / (1 + y1g^2)^(3/2)
  tibble(FPR = xg, TPR = yg, dTPR = y1g, d2TPR = y2g, kappa = kappa)
}

# --- Plotters with small text and single-color line per model ---
plot_curvature <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = kappa)) +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(
      title = paste(model_label, "â€” curvature Îº vs FPR"),
      x = "FPR",
      y = "Îº(FPR)"
    ) +
    coord_cartesian(xlim = c(0, 1)) +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 8),
      panel.grid.minor = element_blank()
    )
}

plot_derivative <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = dTPR)) +
    geom_hline(yintercept = 0, color = "grey80") +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(
      title = paste(model_label, "â€” dTPR/dFPR vs FPR"),
      x = "FPR",
      y = "dTPR/dFPR"
    ) +
    coord_cartesian(xlim = c(0, 1)) +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 8),
      panel.grid.minor = element_blank()
    )
}

# --- Main: build 3x2 grid with curvature on the LEFT, derivative on the RIGHT ---
plot_roc_geometry_grid <- function(smooth_results, n_grid = 1001) {
  # Ensure intended order of rows
  model_order <- c("Decision Tree", "Logistic Regression", "SVM")
  models <- intersect(model_order, unique(smooth_results$model_norm))
  stopifnot(length(models) > 0)

  rows <- map(models, function(m) {
    df <- smooth_results %>% filter(model_norm == m)
    geom_df <- compute_roc_geometry(df, n_grid)
    col_hex <- model_colors[[m]]
    p_left  <- plot_curvature(geom_df, m, col_hex)
    p_right <- plot_derivative(geom_df, m, col_hex)
    p_left | p_right
  })

  # Stack rows into 3x2 (or as many as available)
  reduce(rows, `/`)
}

# --- Usage ---
# smooth_results must have columns: model_norm, FPR, TPR (auc/arc unused here)
grid_plot <- plot_roc_geometry_grid(smooth_results, n_grid = 2001)
print(grid_plot)
```




```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| code-summary: "Show Code"
# # Packages
# library(dplyr)
# library(ggplot2)
# library(purrr)
# library(patchwork)
# library(tibble)

# --- Palette: pastel per model ---
model_colors <- c(
  "Decision Tree"       = "#F28E8E",  # pastel red
  "Logistic Regression" = "#8FD19E",  # pastel green
  "SVM"                 = "#8EB8FF"   # pastel blue
)

# --- Clean ROC per group: sort, drop duplicate FPR, enforce monotone TPR ---
clean_roc <- function(df) {
  df %>%
    arrange(FPR) %>%
    distinct(FPR, .keep_all = TRUE) %>%
    mutate(
      FPR = pmin(pmax(FPR, 0), 1),
      TPR = pmin(pmax(cummax(TPR), 0), 1)
    )
}

# --- Spline builder (use "natural" to avoid strict monotonicity errors) ---
make_spline <- function(df) {
  df <- clean_roc(df)
  splinefun(x = df$FPR, y = df$TPR, method = "natural")
}

# --- Geometry over a uniform grid: y, y', y'', curvature ---
compute_roc_geometry <- function(df, n_grid = 1001) {
  f <- make_spline(df)
  xg  <- seq(0, 1, length.out = n_grid)
  yg  <- f(xg, deriv = 0)
  y1g <- f(xg, deriv = 1)
  y2g <- f(xg, deriv = 2)
  kappa <- abs(y2g) / (1 + y1g^2)^(3/2)
  tibble(FPR = xg, TPR = yg, dTPR = y1g, d2TPR = y2g, kappa = kappa)
}

# --- Plotters with small text and single-color line per model ---
plot_curvature <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = kappa)) +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(
      title = paste(model_label, "â€” curvature Îº vs FPR"),
      x = "FPR",
      y = "Îº(FPR)"
    ) +
    coord_cartesian(xlim = c(0, 1)) +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 8),
      panel.grid.minor = element_blank()
    )
}

plot_derivative <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = dTPR)) +
    geom_hline(yintercept = 0, color = "grey80") +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(
      title = paste(model_label, "â€” dTPR/dFPR vs FPR"),
      x = "FPR",
      y = "dTPR/dFPR"
    ) +
    coord_cartesian(xlim = c(0, 1)) +
    theme_minimal(base_size = 9) +
    theme(
      plot.title = element_text(face = "bold", size = 9),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 8),
      panel.grid.minor = element_blank()
    )
}

# --- Main: build 3x2 grid with curvature on the LEFT, derivative on the RIGHT ---
plot_roc_geometry_grid <- function(smooth_results, n_grid = 1001) {
  # Ensure intended order of rows
  model_order <- c("Decision Tree", "Logistic Regression", "SVM")
  models <- intersect(model_order, unique(smooth_results$model_norm))
  stopifnot(length(models) > 0)

  rows <- map(models, function(m) {
    df <- smooth_results %>% filter(model_norm == m)
    geom_df <- compute_roc_geometry(df, n_grid)
    col_hex <- model_colors[[m]]
    p_left  <- plot_curvature(geom_df, m, col_hex)
    p_right <- plot_derivative(geom_df, m, col_hex)
    p_left | p_right
  })

  # Stack rows into 3x2 (or as many as available)
  reduce(rows, `/`)
}

# --- Usage ---
# smooth_results must have columns: model_norm, FPR, TPR (auc/arc unused here)
grid_plot <- plot_roc_geometry_grid(smooth_results, n_grid = 2001)
print(grid_plot)

```

```{r}
library(dplyr)
library(ggplot2)
library(purrr)
library(patchwork)
library(tibble)

# --- Pastel palette per model ---
model_colors <- c(
  "Decision Tree"       = "#F28E8E",  # pastel red
  "Logistic Regression" = "#8FD19E",  # pastel green
  "SVM"                 = "#8EB8FF"   # pastel blue
)

# --- Clean ROC per group ---
clean_roc <- function(df) {
  df %>%
    arrange(FPR) %>%
    distinct(FPR, .keep_all = TRUE) %>%
    mutate(
      FPR = pmin(pmax(FPR, 0), 1),
      TPR = pmin(pmax(cummax(TPR), 0), 1)
    )
}

# --- Spline builder ---
make_spline <- function(df) {
  df <- clean_roc(df)
  splinefun(x = df$FPR, y = df$TPR, method = "natural")
}

# --- Compute derivatives, curvature, and d(curvature)/dFPR ---
compute_roc_geometry <- function(df, n_grid = 1001) {
  f <- make_spline(df)
  xg  <- seq(0, 1, length.out = n_grid)
  yg  <- f(xg, deriv = 0)
  y1g <- f(xg, deriv = 1)
  y2g <- f(xg, deriv = 2)
  y3g <- f(xg, deriv = 3)   # third derivative for dÎº/dFPR

  kappa <- abs(y2g) / (1 + y1g^2)^(3/2)

  # derivative of curvature wrt FPR (symbolic formula)
  dkappa <- (y3g * (1 + y1g^2) - 3 * y1g * y2g^2) / (1 + y1g^2)^(5/2)

  tibble(FPR = xg, TPR = yg, dTPR = y1g, d2TPR = y2g,
         kappa = kappa, dkappa = dkappa)
}

# --- Plotters ---
plot_curvature <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = kappa)) +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(title = paste(model_label, "â€” curvature Îº(FPR)"),
         x = "FPR", y = "Îº(FPR)") +
    theme_minimal(base_size = 7) +
    theme(plot.title = element_text(face = "bold", size = 7),
          axis.title = element_text(size = 7),
          axis.text  = element_text(size = 7),
          panel.grid.minor = element_blank())
}

plot_derivative <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = dTPR)) +
    geom_hline(yintercept = 0, color = "grey80") +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(title = paste(model_label, "â€” dTPR/dFPR"),
         x = "FPR", y = "dTPR/dFPR") +
    theme_minimal(base_size = 7) +
    theme(plot.title = element_text(face = "bold", size = 7),
          axis.title = element_text(size = 7),
          axis.text  = element_text(size = 7),
          panel.grid.minor = element_blank())
}

plot_dcurvature <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = dkappa)) +
    geom_hline(yintercept = 0, color = "grey80") +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(title = paste(model_label, "â€” dÎº/dFPR"),
         x = "FPR", y = "dÎº/dFPR") +
    theme_minimal(base_size = 7) +
    theme(plot.title = element_text(face = "bold", size = 7),
          axis.title = element_text(size = 7),
          axis.text  = element_text(size = 7),
          panel.grid.minor = element_blank())
}

# --- Main: build 3x3 grid ---
plot_roc_geometry_grid <- function(smooth_results, n_grid = 1001) {
  model_order <- c("Decision Tree", "Logistic Regression", "SVM")
  models <- intersect(model_order, unique(smooth_results$model_norm))
  stopifnot(length(models) > 0)

  rows <- map(models, function(m) {
    df <- smooth_results %>% filter(model_norm == m)
    geom_df <- compute_roc_geometry(df, n_grid)
    col_hex <- model_colors[[m]]
    p_left   <- plot_curvature(geom_df, m, col_hex)
    p_middle <- plot_derivative(geom_df, m, col_hex)
    p_right  <- plot_dcurvature(geom_df, m, col_hex)
    p_left | p_middle | p_right
  })

  reduce(rows, `/`)
}

# --- Usage ---
grid_plot <- plot_roc_geometry_grid(smooth_results, n_grid = 2001)
print(grid_plot)

```


```{r}
library(dplyr)
library(ggplot2)
library(purrr)
library(patchwork)
library(tibble)

# --- Pastel palette per model ---
model_colors <- c(
  "Decision Tree"       = "#F28E8E",  # pastel red
  "Logistic Regression" = "#8FD19E",  # pastel green
  "SVM"                 = "#8EB8FF"   # pastel blue
)

# --- Clean ROC per group ---
clean_roc <- function(df) {
  df %>%
    arrange(FPR) %>%
    distinct(FPR, .keep_all = TRUE) %>%
    mutate(
      FPR = pmin(pmax(FPR, 0), 1),
      TPR = pmin(pmax(cummax(TPR), 0), 1)
    )
}

# --- Spline builder ---
make_spline <- function(df) {
  df <- clean_roc(df)
  splinefun(x = df$FPR, y = df$TPR, method = "natural")
}

# --- Compute curvature and its derivative ---
compute_roc_geometry <- function(df, n_grid = 1001) {
  f <- make_spline(df)
  xg  <- seq(0, 1, length.out = n_grid)
  y1g <- f(xg, deriv = 1)
  y2g <- f(xg, deriv = 2)
  y3g <- f(xg, deriv = 3)

  kappa <- abs(y2g) / (1 + y1g^2)^(3/2)
  dkappa <- (y3g * (1 + y1g^2) - 3 * y1g * y2g^2) / (1 + y1g^2)^(5/2)

  tibble(FPR = xg, kappa = kappa, dkappa = dkappa)
}

# --- Plotters ---
plot_curvature <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = kappa)) +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(title = paste(model_label, "â€” curvature Îº(FPR)"),
         x = "FPR", y = "Îº(FPR)") +
    theme_minimal(base_size = 7) +
    theme(plot.title = element_text(face = "bold", size = 7),
          axis.title = element_text(size = 7),
          axis.text  = element_text(size = 7),
          panel.grid.minor = element_blank())
}

plot_dcurvature <- function(geom_df, model_label, color_hex) {
  ggplot(geom_df, aes(x = FPR, y = dkappa)) +
    geom_hline(yintercept = 0, color = "grey80") +
    geom_line(color = color_hex, linewidth = 0.9) +
    labs(title = paste(model_label, "â€” dÎº/dFPR"),
         x = "FPR", y = "dÎº/dFPR") +
    theme_minimal(base_size = 7) +
    theme(plot.title = element_text(face = "bold", size = 7),
          axis.title = element_text(size = 7),
          axis.text  = element_text(size = 7),
          panel.grid.minor = element_blank())
}

# --- Main: build 3x2 grid (curvature | dÎº/dFPR) ---
plot_roc_geometry_grid <- function(smooth_results, n_grid = 1001) {
  model_order <- c("Decision Tree", "Logistic Regression", "SVM")
  models <- intersect(model_order, unique(smooth_results$model_norm))
  stopifnot(length(models) > 0)

  rows <- map(models, function(m) {
    df <- smooth_results %>% filter(model_norm == m)
    geom_df <- compute_roc_geometry(df, n_grid)
    col_hex <- model_colors[[m]]
    p_left  <- plot_curvature(geom_df, m, col_hex)
    p_right <- plot_dcurvature(geom_df, m, col_hex)
    p_left | p_right
  })

  reduce(rows, `/`)
}

# --- Usage ---
grid_plot <- plot_roc_geometry_grid(smooth_results, n_grid = 2001)
print(grid_plot)

```




**Direct answer:** There is *very little published literature* that explicitly uses **curvature analysis of ROC curves** as a formal diagnostic tool. Most ROC research focuses on the **area under the curve (AUC)**, sensitivity/specificity tradeâ€‘offs, or slope analysis. Curvature is occasionally mentioned in mathematical or geometric treatments of ROC curves, but it is not a mainstream metric in clinical or machine learning practice.

---

## What the literature says

- **Classical ROC analysis**  
  ROC curves are widely studied in statistics, signal detection theory, and clinical epidemiology. The standard metrics are **AUC**, **sensitivity/specificity pairs**, and **likelihood ratios**.

- **Slope and tangent interpretations**  
  Some papers discuss the **slope of the ROC curve** as the likelihood ratio at a given threshold. This is closely related to the derivative of TPR with respect to FPR, which youâ€™ve already been computing. In this sense, slope analysis is wellâ€‘established.

- **Curvature as geometry**  
  Curvature (\(\kappa(x)\)) is not a standard diagnostic measure, but it can be interpreted as a way to **quantify how sharply the ROC bends** at different operating points. This could highlight regions where threshold changes produce disproportionate gains or losses in sensitivity vs specificity. While this idea is mathematically sound, it is rarely formalized in clinical ROC literature.

- **Experimental/novel approaches**  
  Some researchers in machine learning and applied mathematics have explored geometric descriptors of ROC curves (including convexity and curvature) as part of **model comparison** or **shape analysis**, but these are niche and not widely adopted. Most guides (e.g., MedCalc, Statology, MetwareBio) do not mention curvature explicitly.

---

## Why curvature could matter
- **High curvature regions** â†’ thresholds where small changes in FPR produce large changes in TPR (potentially clinically sensitive points).  
- **Low curvature regions** â†’ thresholds where ROC is nearly linear, meaning tradeâ€‘offs are more predictable.  
- **Derivative of curvature** â†’ could identify inflection points where ROC behavior changes most rapidly.

---

## Summary
- **Mainstream ROC literature**: focuses on AUC, slope, sensitivity/specificity.  
- **Curvature analysis**: mathematically valid but rarely used in practice; more of a geometric exploration than a standard clinical metric.  
- **Opportunity**: Your work on curvature and its derivative is pushing into a novel, underexplored area of ROC analysis.

---

Sources: 

---



**Yes, there is literature that discusses analyzing the *derivatives* of smoothed ROC curves, though it is a specialized area. Most of the work comes from mathematical statistics and functional data analysis, where smoothing methods are used to estimate ROC curves and their derivatives for deeper insight into classifier performance.**

---

## Key References

- **Smooth ROC curve estimation via Bernstein polynomials (Wang & Cai, 2021, *PLOS One*)**  
  This paper develops a method to estimate ROC curves using Bernstein polynomial smoothing. Because the estimator is differentiable, it allows computation of **first and second derivatives** of the ROC curve, which can be used to study local properties of the curve beyond AUC.

- **ROCnReg: An R Package for ROC curve inference (RodrÃ­guezâ€‘Ãlvarez & InÃ¡cio, 2021, *R Journal*)**  
  This package implements parametric and nonparametric ROC estimation, including smoothed curves. While the focus is on covariateâ€‘adjusted ROC curves and summary measures, the smoothed functions are differentiable, enabling derivative analysis for threshold optimization.

- **Functional Data Analysis (FDA) approaches (Ramsay & Silverman, 2005; Chow, PSU tutorial)**  
  FDA provides a framework for treating curves (like ROC curves) as functional objects. Derivatives of smoothed curves are explicitly studied to understand local variation and inflection points. This methodology has been applied to ROC curves in some advanced statistical work.

- **Dynamic ROC analysis in survival settings (Heagerty & Zheng, 2005; Geloven et al., 2020)**  
  In timeâ€‘dependent ROC analysis, smoothing is applied to predictors, and derivatives of the smoothed ROC functions are sometimes examined to assess how discrimination changes over time.

---

## Why derivatives matter
- **First derivative (\(d\text{TPR}/d\text{FPR}\))** â†’ slope of the ROC curve, directly interpretable as the likelihood ratio at a threshold.  
- **Second derivative (\(d^2\text{TPR}/d\text{FPR}^2\))** â†’ curvature, quantifies how sharply the ROC bends.  
- **Higher derivatives** â†’ can identify inflection points or regions where classifier performance changes most rapidly.  

These analyses are not mainstream in clinical ROC reporting but are valuable in **simulation studies, threshold optimization, and theoretical exploration**.

---

**In summary:** Yes, there is literature â€” particularly in *PLOS One* (Bernstein polynomial smoothing), the *R Journal* (ROCnReg package), and functional data analysis texts â€” that explicitly enables and sometimes analyzes derivatives of smoothed ROC curves. This is a mathematically rich but niche area compared to standard ROC metrics like AUC.  


---

### ðŸ“š Annotated Bibliography

- **Wang, Y., & Cai, T. (2021). â€œSmooth ROC curve estimation via Bernstein polynomials.â€ *PLOS One*.**  
  Introduces Bernstein polynomial smoothing for ROC curves. Because the estimator is differentiable, it allows computation of **first and second derivatives** of the ROC curve. This makes local slope and curvature analysis possible, beyond global AUC.  
  ðŸ‘‰ Useful if you want a mathematically rigorous framework for derivativeâ€‘based ROC analysis.

- **RodrÃ­guezâ€‘Ãlvarez, M. X., & InÃ¡cio, V. (2021). â€œROCnReg: An R Package for ROC curve inference.â€ *The R Journal*.**  
  Provides parametric and nonparametric ROC estimation methods, including smoothed ROC curves. The differentiable nature of these smoothers enables derivative analysis (e.g., slope at thresholds).  
  ðŸ‘‰ Practical resource if you want to implement derivative analysis in R.

- **Ramsay, J. O., & Silverman, B. W. (2005). *Functional Data Analysis*. Springer.**  
  A foundational text in FDA. Treats curves (like ROC curves) as functional objects, with emphasis on **derivatives of smoothed functions** to study local variation and inflection points.  
  ðŸ‘‰ The theoretical backbone for analyzing ROC derivatives as functional data.

- **Heagerty, P. J., & Zheng, Y. (2005). â€œSurvival model predictive accuracy and ROC curves.â€ *Biometrics*.**  
  In timeâ€‘dependent ROC analysis, smoothing is applied to predictors, and derivatives of smoothed ROC functions are examined to assess how discrimination changes over time.  
  ðŸ‘‰ Shows how derivative analysis can extend to dynamic ROC settings.

- **Geloven, N., et al. (2020). â€œDynamic prediction with timeâ€‘dependent ROC curves.â€ *Statistics in Medicine*.**  
  Explores ROC curves in survival analysis, including smoothed estimates. Derivatives of these curves are used to understand how predictive accuracy evolves.  
  ðŸ‘‰ Demonstrates applied use of smoothed ROC derivatives in clinical prediction.

---

### âœ¨ Takeaway
- **First derivative** â†’ slope of ROC, interpretable as likelihood ratio.  
- **Second derivative** â†’ curvature, quantifies sharpness of tradeâ€‘off.  
- **Higher derivatives** â†’ inflection points, dynamic changes.  

This is a **niche but growing area**: most mainstream ROC work focuses on AUC, but smoothing + derivatives are increasingly used in advanced statistical and machine learning contexts.

---
